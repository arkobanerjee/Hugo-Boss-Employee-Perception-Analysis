{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning Topics to Positive and Negative Reviews using BerTopic Model\n",
    "-------------------\n",
    "\n",
    "> <i>Description: In this notebook, Our Goal was to assign topics to the reviews and compare the topics with GPT base classification.</i>\n",
    "\n",
    "Input Files: \n",
    "1) reviews_merged.csv\n",
    "\n",
    "Output:\n",
    "1) reviews_positive_topics.csv\n",
    "2) reviews_negative_topics.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import umap\n",
    "\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Readin and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_merged = pd.read_csv('reviews_merged.csv')\n",
    "\n",
    "reviews_positive = reviews_merged[['uuid', 'date', 'year', 'rating', 'position'\n",
    "                                   , 'position_code', 'department', 'pros','country', 'file']]\n",
    "reviews_positive = reviews_positive[reviews_positive['pros'].notna()]\n",
    "\n",
    "reviews_negative = reviews_merged[['uuid', 'date', 'year', 'rating', 'position'\n",
    "                                   , 'position_code', 'department', 'cons','country', 'file']]\n",
    "reviews_negative = reviews_negative[reviews_negative['cons'].notna()]\n",
    "print(reviews_merged.shape)\n",
    "print(reviews_positive.shape)\n",
    "print(reviews_negative.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertopic Model For Positive Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for embedding and topic model:\n",
    "\n",
    "* Break down the pros into chunks/sentences to capture multiple Topics in a single review.\n",
    "\n",
    "* lemmatize words, i.e. reducing a word to its base or root form eg. \"working\" or \"worked\" is reduced to \"work\"\n",
    "* finally formating where everything is reduced to lower case, remove empty reviews, remove stopwords and remove anything non alphabetical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\baner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2707\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert NLTK POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN  # Default to noun if POS tag is not recognized\n",
    "\n",
    "# Function to clean and process text without removing stopwords initially\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Remove non-alphabetical characters\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize and POS tagging\n",
    "    words = text.split()\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Lemmatize words based on their POS tags (without removing stopwords initially)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    cleaned_sentence = clean_text(sentence)\n",
    "    \n",
    "    # Remove stopwords after sentence-level splitting\n",
    "    return ' '.join([word for word in cleaned_sentence.split() if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each review into sentences, ensuring stopwords are preserved initially\n",
    "reviews_positive['sentences'] = reviews_positive['pros'].apply(sent_tokenize)\n",
    "\n",
    "# Step 2: Clean sentences after splitting\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Clean each sentence individually after splitting\n",
    "reviews_positive['cleaned_sentences'] = reviews_positive['sentences'].apply(lambda sents: [clean_sentence(sent) for sent in sents])\n",
    "\n",
    "# Explode sentences into individual rows for topic modeling\n",
    "df_sentences = reviews_positive.explode('cleaned_sentences').reset_index(drop=True)\n",
    "filtered_sentences = df_sentences['cleaned_sentences'].tolist()\n",
    "print(len(filtered_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding, Dimensionality Reduction, BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Embedding:** This step allows us to represent words, phrases, or sentences as dense, continuous vectors in a high-dimensional space. These embeddings capture the semantic meaning of the text in a way that is understandable to machine learning models.\n",
    "\n",
    "* **Dimentionality Reduction:** Since embedding generates a high-dimensional vector space, we need to reduce it so that the model can capture patterns optimally, UMAP helps to reduce the dimention without losing much local or global relation.\n",
    "\n",
    "* **BerTopic:** It is a topic modeling technique that combines transformer-based embeddings with clustering methods to discover topics in large sets of text data. It is an advanced and flexible model for extracting coherent topics from textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc309e58bd384cdda8cf36a35783001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic  Count                                               Name  \\\n",
      "0       -1    228      -1_autonomy_lot_lot responsibility_atmosphere   \n",
      "1        0     68                      0_hugo_hugo bos_bos_work hugo   \n",
      "2        1     55            1_fashion_get_industry_fashion industry   \n",
      "3        2     49  2_uniform_free uniform_uniform allowance_commi...   \n",
      "4        3     49  3_culture_work culture_company culture_great c...   \n",
      "..     ...    ...                                                ...   \n",
      "105    104     11  104_international environment_great culture_cu...   \n",
      "106    105     11                  105_medical_insurance_health_food   \n",
      "107    106     11        106_salary_turkey_salary salary_high salary   \n",
      "108    107     11  107_among_cohesion among_cohesion_among colleague   \n",
      "109    108     11  108_benefit discount_discount benefit_benefit_...   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [autonomy, lot, lot responsibility, atmosphere...   \n",
      "1    [hugo, hugo bos, bos, work hugo, bos hugo, bos...   \n",
      "2    [fashion, get, industry, fashion industry, lea...   \n",
      "3    [uniform, free uniform, uniform allowance, com...   \n",
      "4    [culture, work culture, company culture, great...   \n",
      "..                                                 ...   \n",
      "105  [international environment, great culture, cul...   \n",
      "106  [medical, insurance, health, food, plan, medic...   \n",
      "107  [salary, turkey, salary salary, high salary, t...   \n",
      "108  [among, cohesion among, cohesion, among collea...   \n",
      "109  [benefit discount, discount benefit, benefit, ...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [lot responsibility, give friendly welcome imp...  \n",
      "1    [work hugo bos highlight cv, positive work hug...  \n",
      "2    [great learn experience fashion industry pr sk...  \n",
      "3    [employee discount free uniform, commission fr...  \n",
      "4    [great work culture company, good work culture...  \n",
      "..                                                 ...  \n",
      "105  [international environment different culture l...  \n",
      "106  [short friday health insurance without copayme...  \n",
      "107  [high salary level otherwise one would like wo...  \n",
      "108  [good cohesion among employee, good cohesion a...  \n",
      "109  [lot benefit discount voucher etc, extra like ...  \n",
      "\n",
      "[110 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(filtered_sentences, show_progress_bar=True)\n",
    "\n",
    "# Step 4: Apply BERTopic for Topic Modeling at the sentence level\n",
    "# Use UMAP for dimensionality reduction before topic modeling\n",
    "reducer = umap.UMAP(n_neighbors=10, n_components=5, min_dist=0.01, random_state=42)\n",
    "umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(language=\"english\", n_gram_range=(1, 2), calculate_probabilities=True)\n",
    "\n",
    "# Step 5: Fit the BERTopic model to the sentence data\n",
    "topics, probs = topic_model.fit_transform(filtered_sentences, umap_embeddings)\n",
    "\n",
    "# Assign topics and probabilities back to the sentence-level DataFrame\n",
    "df_sentences['topic'] = topics\n",
    "\n",
    "# Option 1: Store the maximum probability for each sentence\n",
    "df_sentences['topic_probability'] = np.max(probs, axis=1)\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                        Name  \\\n",
      "0     -1    228                   -1_good_team_work_company   \n",
      "1      0    745               0_good_great_environment_team   \n",
      "2      1    376                  1_salary_work_good_balance   \n",
      "3      2    372           2_uniform_campus_discount_canteen   \n",
      "4      3    296            3_commission_colleague_good_nice   \n",
      "5      4    286  4_discount_voucher_employee discount_staff   \n",
      "6      5    142                    5_people_work_place_good   \n",
      "7      6    141        6_brand_intern_product_international   \n",
      "8      7     68               7_hugo_bos_hugo bos_work hugo   \n",
      "9      8     53              8_pro_nothing_everything_think   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [good, team, work, company, lot, great, atmosp...   \n",
      "1  [good, great, environment, team, work, company...   \n",
      "2  [salary, work, good, balance, benefit, hour, f...   \n",
      "3  [uniform, campus, discount, canteen, clothing,...   \n",
      "4  [commission, colleague, good, nice, customer, ...   \n",
      "5  [discount, voucher, employee discount, staff, ...   \n",
      "6  [people, work, place, good, place work, good c...   \n",
      "7  [brand, intern, product, international, compan...   \n",
      "8  [hugo, bos, hugo bos, work hugo, work, bos hug...   \n",
      "9  [pro, nothing, everything, think, nothing noth...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [good atmosphere good online course lot traini...  \n",
      "1  [really good work environment good team work g...  \n",
      "2  [good salary good hour good work environment, ...  \n",
      "3  [free uniform, nice uniform good discount, emp...  \n",
      "4       [commission, nice colleague, nice colleague]  \n",
      "5                     [discount, discount, discount]  \n",
      "6  [great place work, good people work, good peop...  \n",
      "7  [well know brand, well know brand product qual...  \n",
      "8  [work hugo bos highlight cv, work hugo bos gre...  \n",
      "9                 [pro, pro, everything nothing pro]  \n"
     ]
    }
   ],
   "source": [
    "topic_model.reduce_topics(filtered_sentences, nr_topics=10)\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics = topic_model.get_topic_info()\n",
    "model_topics.to_csv('reviews_positive_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertopic Model For Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066\n"
     ]
    }
   ],
   "source": [
    "# Split each review into sentences, ensuring stopwords are preserved initially\n",
    "reviews_negative['sentences'] = reviews_negative['cons'].apply(sent_tokenize)\n",
    "\n",
    "# Step 2: Clean sentences after splitting\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Clean each sentence individually after splitting\n",
    "reviews_negative['cleaned_sentences'] = reviews_negative['sentences'].apply(lambda sents: [clean_sentence(sent) for sent in sents])\n",
    "\n",
    "# Explode sentences into individual rows for topic modeling\n",
    "df_sentences = reviews_negative.explode('cleaned_sentences').reset_index(drop=True)\n",
    "filtered_sentences = df_sentences['cleaned_sentences'].tolist()\n",
    "print(len(filtered_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding, Dimensionality Reduction, BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbfc81d81fe4c1cababfcece3e38a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic  Count                                               Name  \\\n",
      "0       -1    322                         -1_tire_promise_store_time   \n",
      "1        0     61         0_management_senior_poor_senior management   \n",
      "2        1     57           1_manager_management_handle_area manager   \n",
      "3        2     52  2_opportunity_opportunity grow_grow_opportunit...   \n",
      "4        3     43   3_balance_worklife_worklife balance_life balance   \n",
      "..     ...    ...                                                ...   \n",
      "121    120     11                   120_gossip_hr_response_encourage   \n",
      "122    121     11  121_stuttgart_metzingen_drive stuttgart_stuttg...   \n",
      "123    122     11  122_drawback_drawback drawback_find drawback_n...   \n",
      "124    123     11               123_none_come mind_none nothing_mind   \n",
      "125    124     10                   124_gift_david_david jones_jones   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [tire, promise, store, time, hour, long, exper...   \n",
      "1    [management, senior, poor, senior management, ...   \n",
      "2    [manager, management, handle, area manager, ha...   \n",
      "3    [opportunity, opportunity grow, grow, opportun...   \n",
      "4    [balance, worklife, worklife balance, life bal...   \n",
      "..                                                 ...   \n",
      "121  [gossip, hr, response, encourage, yet directly...   \n",
      "122  [stuttgart, metzingen, drive stuttgart, stuttg...   \n",
      "123  [drawback, drawback drawback, find drawback, n...   \n",
      "124  [none, come mind, none nothing, mind, many, no...   \n",
      "125  [gift, david, david jones, jones, biscuit, lea...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [wage relatively low due great brand name high...  \n",
      "1    [poor management, overwhelm stressful work atm...  \n",
      "2    [gms report directly regional manager company ...  \n",
      "3    [attract weird people money hungry never work ...  \n",
      "4    [management work life balance, worklife balanc...  \n",
      "..                                                 ...  \n",
      "121  [give supposedly anonymous feedbacksurvery upo...  \n",
      "122  [journey metzingen location associate lot traf...  \n",
      "123  [day still havent find drawback work, dont fin...  \n",
      "124  [nothing thats stand many, nothing come mind r...  \n",
      "125  [little respect personal time though pay good ...  \n",
      "\n",
      "[126 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(filtered_sentences, show_progress_bar=True)\n",
    "\n",
    "# Step 4: Apply BERTopic for Topic Modeling at the sentence level\n",
    "# Use UMAP for dimensionality reduction before topic modeling\n",
    "reducer = umap.UMAP(n_neighbors=10, n_components=5, min_dist=0.01, random_state=42)\n",
    "umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(language=\"english\", n_gram_range=(1, 2), calculate_probabilities=True)\n",
    "\n",
    "# Step 5: Fit the BERTopic model to the sentence data\n",
    "topics, probs = topic_model.fit_transform(filtered_sentences, umap_embeddings)\n",
    "\n",
    "# Assign topics and probabilities back to the sentence-level DataFrame\n",
    "df_sentences['topic'] = topics\n",
    "\n",
    "# Option 1: Store the maximum probability for each sentence\n",
    "df_sentences['topic_probability'] = np.max(probs, axis=1)\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                         Name  \\\n",
      "0     -1    322                      -1_work_hour_store_time   \n",
      "1      0   1011        0_management_company_employee_manager   \n",
      "2      1    726                        1_salary_sale_pay_low   \n",
      "3      2    370                 2_work_balance_location_life   \n",
      "4      3    240                    3_hour_work_long_overtime   \n",
      "5      4    164                     4_con_nothing_none_think   \n",
      "6      5    140          5_metzingen_process_german_decision   \n",
      "7      6     38    6_disadvantage_drawback_negative_downside   \n",
      "8      7     35                7_hugo_hugo bos_bos_work hugo   \n",
      "9      8     20  8_nice unacceptable_na nice_na_unacceptable   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [work, hour, store, time, management, pay, lon...   \n",
      "1  [management, company, employee, manager, staff...   \n",
      "2  [salary, sale, pay, low, commission, pressure,...   \n",
      "3  [work, balance, location, life, life balance, ...   \n",
      "4  [hour, work, long, overtime, day, pay, long ho...   \n",
      "5  [con, nothing, none, think, none none, everyth...   \n",
      "6  [metzingen, process, german, decision, locatio...   \n",
      "7  [disadvantage, drawback, negative, downside, n...   \n",
      "8  [hugo, hugo bos, bos, work hugo, hugo boss, bo...   \n",
      "9  [nice unacceptable, na nice, na, unacceptable,...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [long hour pay good, low pay low commission pe...  \n",
      "1  [company management think make favour letting ...  \n",
      "2  [low salary, low salary low commission sale as...  \n",
      "3  [little respect work life balance, long day ha...  \n",
      "4  [long hour hard work, long hour hard work, lon...  \n",
      "5                [con, con, none none none none con]  \n",
      "6  [location metzingen, long decisionmaking proce...  \n",
      "7  [dont find drawback, much negative downside, d...  \n",
      "8   [con hugo bos, hugo bos hard, con work hugo bos]  \n",
      "9                           [nice, unacceptable, na]  \n"
     ]
    }
   ],
   "source": [
    "topic_model.reduce_topics(filtered_sentences, nr_topics=10)\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics = topic_model.get_topic_info()\n",
    "model_topics.to_csv('reviews_negative_topics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
